{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM(2)_textgen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snl9vFVfuA5I"
      },
      "source": [
        "import numpy\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Dropout\r\n",
        "from keras.layers import LSTM\r\n",
        "from keras.callbacks import ModelCheckpoint\r\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj2JYBGRuH3W",
        "outputId": "c78dbf36-2d05-4322-d4e1-43e0d39e94aa"
      },
      "source": [
        "filename = \"aliceinwonderland.txt\"\r\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\r\n",
        "raw_text = raw_text.lower()\r\n",
        "\r\n",
        "chars = sorted(list(set(raw_text)))\r\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\r\n",
        "\r\n",
        "n_chars = len(raw_text)\r\n",
        "n_vocab = len(chars)\r\n",
        "print (\"Total Characters: \", n_chars)\r\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  144430\n",
            "Total Vocab:  45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPigl4CluLkJ",
        "outputId": "2d975fa6-890e-4c30-f8cf-3617b0e96563"
      },
      "source": [
        "seq_length = 100\r\n",
        "dataX = []\r\n",
        "dataY = []\r\n",
        "for i in range(0, n_chars - seq_length, 1):\r\n",
        "\tseq_in = raw_text[i:i + seq_length]\r\n",
        "\tseq_out = raw_text[i + seq_length]\r\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\r\n",
        "\tdataY.append(char_to_int[seq_out])\r\n",
        "n_patterns = len(dataX)\r\n",
        "print (\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  144330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwsti0cEuNZh"
      },
      "source": [
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\r\n",
        "X = X / float(n_vocab)\r\n",
        "y = np_utils.to_categorical(dataY)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-XfzmBwuOEZ"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(LSTM(256))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5GWfj1BuP_Z"
      },
      "source": [
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\r\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dlvbcKIuTZw",
        "outputId": "3c3b3d86-f100-4258-d8f1-e789c9d2878a"
      },
      "source": [
        "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2256/2256 [==============================] - 54s 20ms/step - loss: 2.9451\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.78825, saving model to weights-improvement-01-2.7883.hdf5\n",
            "Epoch 2/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.4948\n",
            "\n",
            "Epoch 00002: loss improved from 2.78825 to 2.43056, saving model to weights-improvement-02-2.4306.hdf5\n",
            "Epoch 3/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.2500\n",
            "\n",
            "Epoch 00003: loss improved from 2.43056 to 2.21326, saving model to weights-improvement-03-2.2133.hdf5\n",
            "Epoch 4/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.0989\n",
            "\n",
            "Epoch 00004: loss improved from 2.21326 to 2.07442, saving model to weights-improvement-04-2.0744.hdf5\n",
            "Epoch 5/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.9839\n",
            "\n",
            "Epoch 00005: loss improved from 2.07442 to 1.97755, saving model to weights-improvement-05-1.9776.hdf5\n",
            "Epoch 6/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.9033\n",
            "\n",
            "Epoch 00006: loss improved from 1.97755 to 1.90252, saving model to weights-improvement-06-1.9025.hdf5\n",
            "Epoch 7/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.8465\n",
            "\n",
            "Epoch 00007: loss improved from 1.90252 to 1.84427, saving model to weights-improvement-07-1.8443.hdf5\n",
            "Epoch 8/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.8007\n",
            "\n",
            "Epoch 00008: loss improved from 1.84427 to 1.79320, saving model to weights-improvement-08-1.7932.hdf5\n",
            "Epoch 9/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.7479\n",
            "\n",
            "Epoch 00009: loss improved from 1.79320 to 1.74807, saving model to weights-improvement-09-1.7481.hdf5\n",
            "Epoch 10/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.7057\n",
            "\n",
            "Epoch 00010: loss improved from 1.74807 to 1.70572, saving model to weights-improvement-10-1.7057.hdf5\n",
            "Epoch 11/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.6637\n",
            "\n",
            "Epoch 00011: loss improved from 1.70572 to 1.66986, saving model to weights-improvement-11-1.6699.hdf5\n",
            "Epoch 12/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.6370\n",
            "\n",
            "Epoch 00012: loss improved from 1.66986 to 1.63825, saving model to weights-improvement-12-1.6383.hdf5\n",
            "Epoch 13/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.5963\n",
            "\n",
            "Epoch 00013: loss improved from 1.63825 to 1.60433, saving model to weights-improvement-13-1.6043.hdf5\n",
            "Epoch 14/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.5723\n",
            "\n",
            "Epoch 00014: loss improved from 1.60433 to 1.58014, saving model to weights-improvement-14-1.5801.hdf5\n",
            "Epoch 15/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.5445\n",
            "\n",
            "Epoch 00015: loss improved from 1.58014 to 1.55230, saving model to weights-improvement-15-1.5523.hdf5\n",
            "Epoch 16/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.5147\n",
            "\n",
            "Epoch 00016: loss improved from 1.55230 to 1.52843, saving model to weights-improvement-16-1.5284.hdf5\n",
            "Epoch 17/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.4924\n",
            "\n",
            "Epoch 00017: loss improved from 1.52843 to 1.50667, saving model to weights-improvement-17-1.5067.hdf5\n",
            "Epoch 18/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.4776\n",
            "\n",
            "Epoch 00018: loss improved from 1.50667 to 1.48849, saving model to weights-improvement-18-1.4885.hdf5\n",
            "Epoch 19/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.4590\n",
            "\n",
            "Epoch 00019: loss improved from 1.48849 to 1.46626, saving model to weights-improvement-19-1.4663.hdf5\n",
            "Epoch 20/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.4389\n",
            "\n",
            "Epoch 00020: loss improved from 1.46626 to 1.44724, saving model to weights-improvement-20-1.4472.hdf5\n",
            "Epoch 21/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.4159\n",
            "\n",
            "Epoch 00021: loss improved from 1.44724 to 1.43327, saving model to weights-improvement-21-1.4333.hdf5\n",
            "Epoch 22/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.4086\n",
            "\n",
            "Epoch 00022: loss improved from 1.43327 to 1.41851, saving model to weights-improvement-22-1.4185.hdf5\n",
            "Epoch 23/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3909\n",
            "\n",
            "Epoch 00023: loss improved from 1.41851 to 1.40110, saving model to weights-improvement-23-1.4011.hdf5\n",
            "Epoch 24/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3750\n",
            "\n",
            "Epoch 00024: loss improved from 1.40110 to 1.38772, saving model to weights-improvement-24-1.3877.hdf5\n",
            "Epoch 25/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3661\n",
            "\n",
            "Epoch 00025: loss improved from 1.38772 to 1.37559, saving model to weights-improvement-25-1.3756.hdf5\n",
            "Epoch 26/50\n",
            "2256/2256 [==============================] - 45s 20ms/step - loss: 1.3492\n",
            "\n",
            "Epoch 00026: loss improved from 1.37559 to 1.36316, saving model to weights-improvement-26-1.3632.hdf5\n",
            "Epoch 27/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3426\n",
            "\n",
            "Epoch 00027: loss improved from 1.36316 to 1.35205, saving model to weights-improvement-27-1.3521.hdf5\n",
            "Epoch 28/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3234\n",
            "\n",
            "Epoch 00028: loss improved from 1.35205 to 1.33969, saving model to weights-improvement-28-1.3397.hdf5\n",
            "Epoch 29/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3191\n",
            "\n",
            "Epoch 00029: loss improved from 1.33969 to 1.32933, saving model to weights-improvement-29-1.3293.hdf5\n",
            "Epoch 30/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3064\n",
            "\n",
            "Epoch 00030: loss improved from 1.32933 to 1.32050, saving model to weights-improvement-30-1.3205.hdf5\n",
            "Epoch 31/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2991\n",
            "\n",
            "Epoch 00031: loss improved from 1.32050 to 1.31226, saving model to weights-improvement-31-1.3123.hdf5\n",
            "Epoch 32/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2834\n",
            "\n",
            "Epoch 00032: loss improved from 1.31226 to 1.30389, saving model to weights-improvement-32-1.3039.hdf5\n",
            "Epoch 33/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2817\n",
            "\n",
            "Epoch 00033: loss improved from 1.30389 to 1.29509, saving model to weights-improvement-33-1.2951.hdf5\n",
            "Epoch 34/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2791\n",
            "\n",
            "Epoch 00034: loss improved from 1.29509 to 1.28966, saving model to weights-improvement-34-1.2897.hdf5\n",
            "Epoch 35/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2656\n",
            "\n",
            "Epoch 00035: loss improved from 1.28966 to 1.28291, saving model to weights-improvement-35-1.2829.hdf5\n",
            "Epoch 36/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2579\n",
            "\n",
            "Epoch 00036: loss improved from 1.28291 to 1.27358, saving model to weights-improvement-36-1.2736.hdf5\n",
            "Epoch 37/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2542\n",
            "\n",
            "Epoch 00037: loss improved from 1.27358 to 1.27235, saving model to weights-improvement-37-1.2723.hdf5\n",
            "Epoch 38/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2404\n",
            "\n",
            "Epoch 00038: loss improved from 1.27235 to 1.26475, saving model to weights-improvement-38-1.2648.hdf5\n",
            "Epoch 39/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.2461\n",
            "\n",
            "Epoch 00039: loss improved from 1.26475 to 1.25919, saving model to weights-improvement-39-1.2592.hdf5\n",
            "Epoch 40/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 1.3503\n",
            "\n",
            "Epoch 00040: loss did not improve from 1.25919\n",
            "Epoch 41/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.6415\n",
            "\n",
            "Epoch 00041: loss did not improve from 1.25919\n",
            "Epoch 42/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.7233\n",
            "\n",
            "Epoch 00042: loss did not improve from 1.25919\n",
            "Epoch 43/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.5946\n",
            "\n",
            "Epoch 00043: loss did not improve from 1.25919\n",
            "Epoch 44/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.4926\n",
            "\n",
            "Epoch 00044: loss did not improve from 1.25919\n",
            "Epoch 45/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.3987\n",
            "\n",
            "Epoch 00045: loss did not improve from 1.25919\n",
            "Epoch 46/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.3036\n",
            "\n",
            "Epoch 00046: loss did not improve from 1.25919\n",
            "Epoch 47/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.2105\n",
            "\n",
            "Epoch 00047: loss did not improve from 1.25919\n",
            "Epoch 48/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.1454\n",
            "\n",
            "Epoch 00048: loss did not improve from 1.25919\n",
            "Epoch 49/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.0643\n",
            "\n",
            "Epoch 00049: loss did not improve from 1.25919\n",
            "Epoch 50/50\n",
            "2256/2256 [==============================] - 46s 20ms/step - loss: 2.0097\n",
            "\n",
            "Epoch 00050: loss did not improve from 1.25919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f851e620198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X03kL0hIFd4N",
        "outputId": "6c52c625-3768-4399-ec75-65a0a834a130"
      },
      "source": [
        "filename = \"weights-improvement-39-1.2592.hdf5\"\r\n",
        "model.load_weights(filename)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\r\n",
        "\r\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\r\n",
        "start = numpy.random.randint(0, len(dataX)-1)\r\n",
        "pattern = dataX[start]\r\n",
        "print(\"Seed:\")\r\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\r\n",
        "# generate characters\r\n",
        "generated_text = \"\"\r\n",
        "for i in range(1000):\r\n",
        "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\r\n",
        "\tx = x / float(n_vocab)\r\n",
        "\tprediction = model.predict(x, verbose=0)\r\n",
        "\tindex = numpy.argmax(prediction)\r\n",
        "\tresult = int_to_char[index]\r\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\r\n",
        "\tgenerated_text = generated_text + result\r\n",
        "\tpattern.append(index)\r\n",
        "\tpattern = pattern[1:len(pattern)]\r\n",
        "print(generated_text)\r\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" d--(she considered him to be a footman because he was in livery:\n",
            "otherwise, judging by his face only \"\n",
            " a cabke in the door, and she tented to be she was going to say the thing of the soom, and she was quite all the white rabbit read of the boorused would have got so she sable, but the doom was seeming the distance with the soof, 'and they dould not ae an old so she shate of the same with the sea. the master was she sable of the court with a little belor her beak to the thme, and was quite a little billeren the white rabbit, who was not a dorrer with the cane, whth a little shmence, and then she was see any rate, which she was not a loment to be talking at the cook, and the thing was so like to her tery such a cand and taie, with a she white rabbit, who was not a dorrer with the cane, whth a little shmence, and the tor of the court, with a shall had to be she was not a dishgt with to say that she was so tat the white rabbit, and she tenten the white rabbit read out of the soo of the court, and the thing was so like the dance.\n",
            "\n",
            "'i mav the sable bs the sea.' said the mock turtle. 'but i d\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}